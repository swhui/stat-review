{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra Review\n",
    "\n",
    "In this section, I will add problems reviewing linear algebra concepts that are helpful to know in statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notation\n",
    "\n",
    "A scalar is a real number in $\\mathbb {R}$. A vector is a list or array of real numbers in $\\mathbb{R}^{d}$ where $d$ is the length of the list/array. A matrix is a 2-dimensional array of numbers. We will denote scalars with lowercase unbolded numbers ($c$), vectors with lowercase bolded letters ($\\pmb{v}$), and matrices with uppercase unbolded letters ($X$).\n",
    "\n",
    "\n",
    "\n",
    "## Basic Operations for Vectors\n",
    "\n",
    "Let $\\pmb {v}$ be a $d$-dimensional vector ($\\pmb {v} \\in {\\mathbb R}^{d}$). Denote the entries of the vector to be: \n",
    "$\\pmb{v} = \\begin{pmatrix} v_{1} \\\\ \\vdots \\\\ v_{d} \\end{pmatrix}$\n",
    "\n",
    "### Addition\n",
    "\n",
    "Let $\\pmb{v} \\in \\mathbb{R}^{d}$ and $\\pmb{w} \\in \\mathbb{R}^{d}$. $(\\pmb{v} + \\pmb{w})_{i} = \\pmb{v}_{i} + \\pmb{w}_{i}$, where $\\pmb{v}+\\pmb{w} \\in \\mathbb{R}^{d}$. More explicitly, we have\n",
    "\n",
    "$$\\pmb{v} + \\pmb{w} = \\begin{pmatrix} v_{1} \\\\ \\vdots \\\\ v_{d} \\end{pmatrix} + \\begin{pmatrix} w_{1} \\\\ \\vdots \\\\ w_{d} \\end{pmatrix} = \\begin{pmatrix} v_{1} + w_{1} \\\\ \\vdots \\\\ v_{d} + w_{d} \\end{pmatrix}$$\n",
    "\n",
    "\n",
    "### Scalar multiplication\n",
    "\n",
    "When we multiply a scalar $c$ to $\\pmb v$, we have that $c\\pmb{v}$ is also a real vector whose entries are $(c\\pmb{v})_{i} = c\\pmb{v}_{i}$ for $i \\in [1,{d}]$.\n",
    "\n",
    "### Transpose\n",
    "\n",
    "The transpose of $\\pmb{v}$ is $\\pmb{v}' =  \\begin{pmatrix} v_{1} & ... & v_{d} \\end{pmatrix}$.\n",
    "\n",
    "### Multiplication of two vectors\n",
    "\n",
    "Inner product: Let $\\pmb{v} \\in \\mathbb{R}^{d}$ and $\\pmb{w} \\in \\mathbb{R}^{d}$. Then $\\pmb{v} \\cdot \\pmb{w} = \\pmb{v}'\\pmb{w} = \\sum_{i=1}^{d} v_{i}w_{i}$. The result of an inner product between two vectors results in a scalar, and the dimensions of the two vectors must be equal.\n",
    "\n",
    "$$\\pmb{v}' \\pmb{w} = \\begin{pmatrix} v_{1}  & ... & v_{d} \\end{pmatrix}  \\begin{pmatrix} w_{1} \\\\ \\vdots \\\\ w_{d} \\end{pmatrix} = v_{1}  w_{1} + v_{2}  w_{2} + ... + v_{d} w_{d}$$\n",
    "\n",
    "Outer product: Let $\\pmb{v} \\in \\mathbb{R}^{d}$ and $\\pmb{w} \\in \\mathbb{R}^{d}$. Then the outer product is $\\pmb{v}\\pmb{w}'$, which is a $d \\times d$ matrix whose entry on the $i$th row and $j$th column is $(\\pmb{v}\\pmb{w}')_{ij} = v_{i}w_{j}$.\n",
    "\n",
    "\n",
    "$$\\pmb{v}\\pmb{w}' = \\begin{pmatrix} v_{1} \\\\ \\vdots \\\\ v_{d} \\end{pmatrix} \\begin{pmatrix} w_{1} & ... & w_{d} \\end{pmatrix} =\\begin{pmatrix} v_{1} w_{1}  & v_{1} w_{2}&... & v_{1} w_{d} \\\\ v_{2} w_{1} &  v_{2} w_{2} & ... & v_{2} w_{d} \\\\ \\vdots &  \\vdots & \\ddots & \\vdots \\\\ v_{d} w_{1} &  v_{d} w_{2} & ... & v_{d} w_{d}  \\end{pmatrix} $$\n",
    "\n",
    "### Norm\n",
    "\n",
    "The norm of a vector is the inner product of a vector $\\pmb{v}$ and itself. $\\| \\pmb{v} \\|_{2} = \\sqrt{\\sum_{i=1}^{d}v_{i}^{2}}$\n",
    "\n",
    "\n",
    "## Basic Operations for Matrices \n",
    "\n",
    "Let $c$ be a scalar. \n",
    "\n",
    "### Addition\n",
    "\n",
    "Let $A$ be a $n \\times m$ matrix and $B$ be a $n \\times m$ matrix. Then\n",
    "\n",
    "$$A+B = \n",
    "\\begin{pmatrix} \n",
    "a_{11} & a_{12} & ... & a_{1m} \\\\ \n",
    "a_{21} & a_{22} & ... & a_{2m} \\\\ \n",
    "\\vdots &\\vdots  & \\ddots & \\vdots  \\\\ \n",
    "a_{n1} & a_{n2} & ... & a_{nm} \\end{pmatrix} + \n",
    "\\begin{pmatrix} \n",
    "b_{11} & b_{12} & ... & b_{1m} \\\\ \n",
    "b_{21} & b_{22} & ... & b_{2m} \\\\ \n",
    "\\vdots &\\vdots  & \\ddots & \\vdots  \\\\ \n",
    "b_{n1} & b_{n2} & ... & b_{nm} \\end{pmatrix}\n",
    " = \n",
    "\\begin{pmatrix} \n",
    "a_{11} + b_{11} & a_{12} + b_{12} & ... & a_{1m} + b_{1m} \\\\ \n",
    "a_{21} + b_{21} & a_{22} + b_{22} & ... & a_{2m} + b_{2m} \\\\ \n",
    "\\vdots & \\vdots  & \\ddots & \\vdots  \\\\ \n",
    "a_{n1} + b_{n1} & a_{n2} + b_{n2} & ... & a_{nm}+ b_{nm} \\end{pmatrix}$$\n",
    "\n",
    "### Scalar multiplication\n",
    "\n",
    "$$cA = \\begin{pmatrix} \n",
    "ca_{11} & ca_{12} & ... & ca_{1m} \\\\ \n",
    "ca_{21} & ca_{22} & ... & ca_{2m} \\\\ \n",
    "\\vdots &\\vdots  & \\ddots & \\vdots  \\\\ \n",
    "ca_{n1} & ca_{n2} & ... & ca_{nm} \\end{pmatrix}$$\n",
    "\n",
    "### Transpose\n",
    "\n",
    "The transpose of $A$, denoted as $A'$ is:\n",
    "\n",
    "$$A' = \n",
    "\\begin{pmatrix} \n",
    "a_{11} & a_{12} & ... & a_{n1} \\\\ \n",
    "a_{12} & a_{22} & ... & a_{n2} \\\\ \n",
    "\\vdots &\\vdots  & \\ddots & \\vdots  \\\\ \n",
    "a_{1m} & a_{2m} & ... & a_{nm} \\end{pmatrix}$$\n",
    "\n",
    "### Multiplication\n",
    "\n",
    "Multiplying a matrix and a vector:\n",
    "\n",
    "$$A \\pmb{v} = \\begin{pmatrix} \n",
    "a_{11} & a_{12} & ... & a_{1m} \\\\ \n",
    "a_{21} & a_{22} & ... & a_{2m} \\\\ \n",
    "\\vdots &\\vdots  & \\ddots & \\vdots  \\\\ \n",
    "a_{n1} & a_{n2} & ... & a_{nm} \\end{pmatrix} \\begin{pmatrix} v_{1} \\\\ \\vdots \\\\ v_{d} \\end{pmatrix}\n",
    "= \\begin{pmatrix} \n",
    "a_{11} v_{1} + a_{12} v_{2} + ... + a_{1m} v_{d}\\\\ \n",
    "a_{21} v_{1} + a_{22} v_{2} + ... + a_{2m} v_{d}\\\\ \n",
    "\\vdots  \\\\ \n",
    "a_{n1} v_{1} + a_{n2} v_{2} + ... + a_{nm} v_{d}\\end{pmatrix}$$\n",
    "\n",
    "Multiplying a matrix and another matrix:\n",
    "\n",
    "Let $A$ be a $n \\times m$ matrix and $B$ be a $m \\times p$ matrix.\n",
    "\n",
    "$$AB = \n",
    "\\begin{pmatrix} \n",
    "a_{11} & a_{12} & ... & a_{1m} \\\\ \n",
    "a_{21} & a_{22} & ... & a_{2m} \\\\ \n",
    "\\vdots &\\vdots  & \\ddots & \\vdots  \\\\ \n",
    "a_{n1} & a_{n2} & ... & a_{nm} \\end{pmatrix} \n",
    "\\begin{pmatrix} \n",
    "b_{11} & b_{12} & ... & b_{1} \\\\ \n",
    "b_{21} & b_{22} & ... & b_{2p} \\\\ \n",
    "\\vdots &\\vdots  & \\ddots & \\vdots \\\\ \n",
    "b_{m1} & b_{m2} & ... & b_{mp} \\end{pmatrix}= \n",
    "\\begin{pmatrix} \n",
    "\\sum_{k=1}^{d} a_{1k} b_{k1} & \\sum_{k=1}^{d} a_{1k} b_{k2} & ... & \\sum_{k=1}^{d} a_{1k} b_{km} \\\\ \n",
    "\\sum_{k=1}^{d} a_{2k} b_{k1} & \\sum_{k=1}^{d} a_{2k} b_{k2} & ... & \\sum_{k=1}^{d} a_{2k} b_{km} \\\\ \n",
    "\\vdots &\\vdots  & \\ddots & \\vdots  \\\\ \n",
    "\\sum_{k=1}^{d} a_{nk} b_{k1} & \\sum_{k=1}^{d} a_{nk} b_{k2}& ... &  \\sum_{k=1}^{d} a_{nk} b_{kp} \\end{pmatrix} \n",
    "$$\n",
    "\n",
    "Also, matrix multiplication is distributive but not commutative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse\n",
    "\n",
    "If $A \\in \\mathbb{R}^{d \\times d}$, $A^{-1}$ is a $d \\times d$ matrix such that $AA^{-1} = A^{-1} A=I_{d}$.\n",
    "\n",
    "Let $A, B$ be invertible matrices. Then the product of two invertible matrices is also invertible and\n",
    "\n",
    "$$(AB)^{-1} = B^{-1}A^{-1}$$\n",
    "\n",
    "### Inverting a $2 \\times 2$ matrix: \n",
    "\n",
    "Let $A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$.\n",
    "Then, we have that $A^{-1} = \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$.\n",
    "\n",
    "\n",
    "### Special Matrices\n",
    "\n",
    " - **Square matrices:** If the number of rows equals the number of columns, then the matrix is square\n",
    "\n",
    " - **Idempotent matrices:** If $A^{2} = A$, then $A$ is idempotent.\n",
    "\n",
    " - **Symmetric matrices:** If $A^{T} = A$, then $A$ is symmetric.\n",
    "\n",
    " - **Diagonal matrices:** If $A_{ij} = 0$ whenever $i \\neq j$ (off the diagonal from upper left to bottom right), then $A$ is diagonal.\n",
    "\n",
    " - **Identity matrices:** A special square, symmetric, and diagonal matrix is the identity matrix, or a matrix whose only nonzero entries are 1's along the diagonal.\n",
    " \n",
    " - **Positive definite matrices:** Let $A$ be a $d\\times d$ symmetric matrix such that $\\pmb{v}' A {\\pmb v} > 0$ for all nonzero $\\pmb{ v} \\in {\\mathbb R}^{d}$. Then $A$ is positive definite.\n",
    "\n",
    " - **Positive semi-definite matrices:** Let $A$ be a $d\\times d$ symmetric matrix such that $\\pmb{v}' A {\\pmb v} \\geq 0$ for all nonzero $\\pmb{ v} \\in {\\mathbb R}^{d}$. Then $A$ is positive semi-definite.\n",
    " \n",
    " - **Negative definite matrices:** Let $A$ be a $d\\times d$ symmetric matrix such that $\\pmb{v}' A {\\pmb v} < 0$ for all nonzero $\\pmb{ v} \\in {\\mathbb R}^{d}$. Then $A$ is negative definite.\n",
    "\n",
    " - **Negative semi-definite matrices:** Let $A$ be a $d\\times d$ symmetric matrix such that $\\pmb{v}' A {\\pmb v} \\leq 0$ for all nonzero $\\pmb{ v} \\in {\\mathbb R}^{d}$. Then $A$ is negative semi-definite.\n",
    " \n",
    "### Eigenvalues and eigenvectors\n",
    "\n",
    "If $A \\pmb v = c \\pmb v$ where $A$ is a $n \\times d$ matrix and $\\pmb v$ is a nonzero vector in $\\mathbb{R}^{d}$, then $c$ is an eigenvalue of $A$ with corresponding eigenvector $\\pmb v$.\n",
    "\n",
    "Note: The space of eigenvectors corresponding to eigenvalue $0$ are in the null space or kernel of the matrix $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algebra Exercises\n",
    "\n",
    "Find the eigenvalues and eigenvectors of $X$ where\n",
    "$$X = \\begin{pmatrix} -1 & 4 \\\\ -2 & 5\\end{pmatrix}$$\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "$$X - \\lambda I = \\begin{pmatrix} -1-\\lambda & 4 \\\\ -2 & 5 -\\lambda \\end{pmatrix}$$\n",
    "\n",
    "$$det( X - \\lambda I ) = ( -1-\\lambda )( 5 -\\lambda ) + 8 = -5 - 4\\lambda + \\lambda^{2} + 8 = \\lambda^{2} -4\\lambda +3 = (\\lambda -3)(\\lambda - 1)$$\n",
    "\n",
    "Then, the eigenvalues are $3$ and $1$.\n",
    "\n",
    "To find our eigenvectors, $v_{1}, v_{2}$, we use the fact that $Xv_{1} = 3v_{1}$ and $X v_{2} = 1 v_{2}$\n",
    "\n",
    "$$\\begin{pmatrix} -1 & 4 \\\\ -2 & 5\\end{pmatrix} \\begin{pmatrix} v_{11}  \\\\ v_{12}\\end{pmatrix} = \\begin{pmatrix} 3v_{11}  \\\\ 3v_{12}\\end{pmatrix}$$\n",
    "\n",
    "$$\\begin{pmatrix} -1 & 4 \\\\ -2 & 5\\end{pmatrix} \\begin{pmatrix} v_{21}  \\\\ v_{22}\\end{pmatrix} = \\begin{pmatrix} v_{21}  \\\\ v_{22}\\end{pmatrix}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra in Statistics\n",
    "\n",
    "We will extend the notion of a random variable into random vectors. The random vector is a vectors where each entry of the vector is a random variable, and a random matrix is a matrix where each entry of the matrix is a random variable. Just like how we can find the expectation of a random variable, we can find the expectation of a random vector.\n",
    "\n",
    "Let $X$ be a random vector with dimension $d$ ($X \\in \\mathbb{R}^{d}$). In other words, let $X = \\begin{pmatrix} X_{1} \\\\ \\vdots \\\\ X_{d} \\\\ \\end{pmatrix}$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\\mathbb{E}[X] = \\mathbb{E}  \\begin{bmatrix} X_{1} \\\\ \\vdots \\\\ X_{d} \\\\ \\end{bmatrix}  = \\begin{pmatrix} \\mathbb{E}[X_{1}] \\\\ \\vdots \\\\ \\mathbb{E}[X_{d}]  \\end{pmatrix}$$\n",
    "\n",
    "Furthermore, the expectation of a random matrix $X \\in \\mathbb{R}^{d \\times d}$ as\n",
    "the matrix whose entries are the expectations of the entries of $X$, and the $ij$-th entry would be $(\\mathbb{E}[X])_{ij} = \\mathbb{E}[X_{ij}]$.\n",
    "\n",
    "\n",
    "We can also define a covariance of a random vector. Let $X \\in \\mathbb{R}^{d}$. Then $Cov(X) = Cov(X,X) = \\mathbb{E}[(X- \\mathbb{E}(X) )(X- \\mathbb{E}(X) )']$. We can use matrix multiplication and express this in terms of the individual components of $X$ as follows:\n",
    "\n",
    "$$Var(X) = Cov(X, X) = \\begin{bmatrix} \\mathbb{E}(X_{1} - \\mathbb{E}(X_{1}))^{2} & \\dots & \\mathbb{E}(X_{1} - \\mathbb{E}(X_{1}) )(X_{d} - \\mathbb{E}(X_{d})) \\\\\n",
    "\\dots & \\dots & \\dots \\\\ \\mathbb{E}(X_{d} - \\mathbb{E}(X_{d}))(X_{1} - \\mathbb{E}(X_{1})) & \\dots & \\mathbb{E}(X_{d} - \\mathbb{E}(X_{d}))^{2} \\end{bmatrix}$$\n",
    "\n",
    "Furthermore, consider $X \\in \\mathbb{R}^{d}$ and $Y \\in \\mathbb{R}^{p}$. Then $Cov(X, Y) = \\mathbb{E}[(X- \\mathbb{E}(X) )(Y- \\mathbb{E}(Y) )']$.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Let $A$ be a $n \\times d$ constant matrix and $X$ be a $d$-dimensional random vector. Then $E[AX]= AE[X]$.\n",
    "\n",
    "Furthermore, $Cov(AX) = A Cov(X) A'$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Multiple Linear Regression\n",
    "\n",
    "In simple linear regression, we predict an outcome variable from one predictor variable. However, in multiple linear regression, we predict an outcome variable from several predictor variables. In this set up, we assume that each observation's response variable can be found by some linear combination of many predictors, plus an error term. In particular, we believe that for the $i$-th observation in our sample, $y_{i} = \\beta_{0} + \\beta_{1}x_{i, 1} + \\dots + \\beta_{d}x_{i, d} + \\epsilon_{i}$ where $y_{i}$ is the $i$th observation's value of the response variable, $x_{i, 1}, \\dots, x_{i, d}$ are the $i$-th observation's predictor variables, and $\\epsilon_{i}$ is the error.\n",
    "\n",
    "\n",
    "Now, assume we have $n$ observations in our sample, and $d$ variables of interest. We can represent our problem using matrix notation: \n",
    "\n",
    "$$\\pmb{Y} = X\\pmb{\\beta} + \\pmb{\\epsilon}$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\\pmb{Y} = \\begin{bmatrix} y_{1} \\\\ \\dots \\\\ y_{n} \\end{bmatrix}, X = \\begin{bmatrix} 1 & x_{1,1} & \\dots & x_{1,d} \\\\ 1 & x_{2,1} & \\dots & x_{2,d} \\\\ \\dots & \\dots & \\dots & \\dots \\\\ 1 & x_{n,1} & \\dots & x_{n,d} \\end{bmatrix}, \\pmb{\\beta} = \\begin{bmatrix} \\beta_{0} \\\\ \\dots \\\\ \\beta_{d} \\end{bmatrix} \\pmb{\\epsilon} = \\begin{bmatrix} \\epsilon_{1} \\\\ \\dots \\\\ \\epsilon_{n} \\end{bmatrix}$$\n",
    "    \n",
    "Since we have $d$ predictors, we want to find $\\beta_{0}, \\beta_{1}, ..., \\beta_{d}$ such that we minimize some cost function (let it be the sum of squared errors for convenience): $\\sum_{i=1}^{n}(y_{i} - (\\beta_{0} + \\beta_{1}x_{i,1} + \\dots + \\beta_{d}x_{i,d}))^{2}$.\n",
    "\n",
    "We can use matrix calculus to analytically solve for $\\hat \\beta_{0}, \\dots, \\hat{\\beta}_{d}$. \n",
    "\n",
    "We can then show that $$\\hat{\\pmb{\\beta}} = (X^{T}X)^{-1}X^{T}\\pmb{Y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Show that $Cov(AX) = A Cov(X) A'$:\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "$$Cov(AX) = { \\mathbb E}[(AX-\\mathbb E[AX](AX-\\mathbb E[AX]))'] = \\mathbb E[A(X-\\mathbb E[X]) (X-\\mathbb E[X])'A' ]= A \\mathbb{E}[(X- \\mathbb{E}(X) )(X- \\mathbb{E}(X) )'] A' = A Cov(X) A'$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
