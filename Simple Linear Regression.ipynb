{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression\n",
    "\n",
    "\n",
    "The most common reason that we want produce a simple linear regression model is when we want to represent a relationship between some outcome variable and input variable(s). We may do this to predict the outcome variable based on the input variables (prediction) or understand the relationship between the outcome variable and the input variables (inference).\n",
    "\n",
    "We assume that each $i$-th observation can be represented as: \n",
    "\n",
    "$$y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\epsilon_{i}$$\n",
    "\n",
    "We can see that this relationship is linear with respect to the coefficients ($\\beta_{j}$ terms).\n",
    "\n",
    "The first part, $\\beta_{0} + \\beta_{1} x_{i}$, gives a general linear relationship that describes the relationship between $y$ and $x$ for all observations. The error term $\\epsilon_{i}$ allows for there to be variation from the linear trend. Our prediction for the $i$-th observation would be ${\\hat y}_{i} = \\beta_{0} + \\beta_{1} x_{i}$.\n",
    "\n",
    "The most basic assumptions for a linear regression are: $\\epsilon_{i}$ is random and the $x_{i}$ are fixed.\n",
    "\n",
    "Generally, we also assume that the $\\epsilon_{i}$ are IID and $\\mathbb{E} (\\epsilon_{i})=0$ and $Var(\\epsilon_{i}) = \\sigma^{2}$ for all $i$ observations.\n",
    "\n",
    "Notice that in simple linear regression, we only have one predictor variable of interest.\n",
    "\n",
    "Our predictions for our outcome variable will usually be higher, lower, or equal to the actual $y_{i}$ value and $y_{i} - {\\hat y}_{i}$ is our error. Now, given our input and output variables, we want to find the $\\beta_{0}$ and $\\beta_{1}$ that minimize some cost/loss function, which quantifies how off our predictions are from the actual values. The two most common are the sum (or mean) of squared errors and the sum (or mean) of absolute errors.\n",
    "\n",
    "Sum of squared errors:\n",
    "\n",
    "$$\\sum_{i=1}^{n} (y_{i} - {\\hat y}_{i} )^{2} = \\sum_{i=1}^{n} (y_{i} - (\\beta_{0} + \\beta_{1} x_{i} ) )^{2}$$\n",
    "\n",
    "\n",
    "Sum of absolute errors:\n",
    "\n",
    "$$\\sum_{i=1}^{n} | (y_{i} - {\\hat y}_{i} |$$\n",
    "\n",
    "Again, the goal is the choose $\\beta_{0}$ and $\\beta_{1}$ such that we minimize the loss/cost function (SSE or SAE).\n",
    "\n",
    "If our cost function be the sum of squared errors, we can write ${\\hat \\beta_{0}} = {\\arg\\min}_{\\beta_{0}} \\sum_{i=1}^{n} (y_{i} - (\\beta_{0} + \\beta_{1} x_{i} ) )^{2}$ and ${\\hat \\beta_{1}} = \\arg\\min_{\\beta_{1}} \\sum_{i=1}^{n} (y_{i} - (\\beta_{0} + \\beta_{1} x_{i} ) )^{2}$\n",
    "\n",
    "Using calculus, we can show that ${\\hat \\beta}_{0} = {\\bar y} - {\\hat \\beta}_{1} {\\bar x}$ and ${\\hat \\beta}_{1} = \\frac{ (y_{i} - {\\bar y})( x_{i} - {\\bar x} ) }{ \\sum_{i} (x_{i} - {\\bar x})^{2} } = \\frac{Cov(x, y)}{ Var(x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1 \n",
    "\n",
    "Show that ${\\hat \\beta}_{0} = {\\bar y} - {\\hat \\beta}_{1} {\\bar x}$ and ${\\hat \\beta}_{1} = \\frac{ (y_{i} - {\\bar y})( x_{i} - {\\bar x} ) }{ \\sum_{i} (x_{i} - {\\bar x})^{2} } = \\frac{Cov(x, y)}{ Var(x)}$\n",
    "\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "Let $S = \\sum_{i=1}^{n} (y_{i} - (\\beta_{0} + \\beta_{1} x_{i} ) )^{2}$\n",
    "\n",
    "Take the derivative of $S$ with respect to $\\beta_{0}$ and $\\beta_{1}$, set to 0, and solve for $\\beta_{0}$ and $\\beta_{1}$ respectively.\n",
    "\n",
    "$$\\frac{\\partial S}{\\partial \\beta_{0}} = -2 \\sum_{i=1}^{n} (y_{i} - \\beta_{0} - \\beta_{1} x_{i}) \\overset{set}{=} 0$$\n",
    "\n",
    "With some algebra, ${\\beta}_{0} = {\\bar y} - \\beta_{1} {\\bar x}$. Since we do not know the true $\\beta_{1}$, we estimate with ${\\hat \\beta}_{1}$. Then, we have ${\\hat \\beta}_{0} = {\\bar y} - {\\hat \\beta}_{1} {\\bar x}$\n",
    "\n",
    "$$\\frac{\\partial S}{\\partial \\beta_{1}} = -2 \\sum_{i=1}^{n} x_{i}(y_{i} - \\beta_{0} - \\beta_{1} x_{i}) \\overset{set}{=} 0$$\n",
    "\n",
    "$$0=\\sum_{i=1}^{n} x_{i} y_{i} - {\\beta}_{0} \\sum_{i=1}^{n} x_{i} - \\beta_{1} \\sum_{i=1}^{n} x_{i}^{2}$$\n",
    "\n",
    "Using the previous condition, ${\\beta}_{0} = {\\bar y} - \\beta_{1} {\\bar x}$, substitute accordingly.\n",
    "\n",
    "$$0=\\sum_{i=1}^{n} x_{i} y_{i} -({\\bar y} - \\beta_{1} {\\bar x}) \\sum_{i=1}^{n} x_{i} - \\beta_{1} \\sum_{i=1}^{n} x_{i}^{2}$$\n",
    "\n",
    "Rearranging terms, we find \n",
    "\n",
    "$${\\hat \\beta}_{1} = \\frac{ \\sum_{i=1}^{n} x_{i} y_{i} - {\\bar y} \\sum_{i=1}^{n} x_{i} }{ \\sum_{i=1}^{n} x_{i}^{2} - {\\bar x} \\sum_{i=1}^{n} x_{i} } = \\frac{ \\sum_{ i=1 }^{n} ( x_{i} - {\\bar x} )(y_{i} - {\\bar y}) }{ \\sum_{i=1}^{n} ( x_{i}- {\\bar x})^{2}}$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "To get the top and bottom to look the same, notice that $\\sum y_{i} = n{\\bar y}$ and $\\sum x_{i} = n{\\bar x}$:\n",
    "\n",
    "$$\\sum_{ i=1 }^{n} ( x_{i} - {\\bar x} )(y_{i} - {\\bar y}) = \\sum_{ i=1 }^{n} x_{i} y_{i} - \\sum_{ i=1 }^{n} {\\bar x} y_{i} - \\sum_{ i=1 }^{n} {\\bar y} x_{i} + n {\\bar x} {\\bar y} = \\sum_{ i=1 }^{n} x_{i} y_{i}  - \\sum_{ i=1 }^{n} {\\bar y} x_{i} - n{\\bar y} {\\bar x}+n{\\bar y} {\\bar x}$$\n",
    "\n",
    "You can also verify that $\\sum_{i=1}^{n} x_{i}^{2} - {\\bar x} \\sum_{i=1}^{n} x_{i} = \\sum_{i=1}^{n} ( x_{i}- {\\bar x})^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
