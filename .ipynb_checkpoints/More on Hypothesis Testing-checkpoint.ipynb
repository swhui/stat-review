{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing\n",
    "\n",
    "In general, a hypothesis tests examines the validity of a null hypothesis versus and alternative hypothesis about some unknown parameter of interest.\n",
    "\n",
    "For example, suppose I have a sample, $X_{1},\\dots, X_{n}$ from some distribution parametrized by an unknown $\\theta$, and we wish to test $$H_{0}: \\theta = 0 \\text{   vs.  } H_{1}: \\theta \\neq 0$$ \n",
    "Then, we establish some test statistic, $T(X_{1}, \\dots, X_{n})$. If the observed test statistic looks unlikely to\n",
    "have come from its null distribution (assuming $\\theta = 0$), we reject in favor of the alternative. Otherwise, we fail to reject.\n",
    "\n",
    "A simple hypothesis is one in which the parameter of interest can only take on one value. Above, the null hypothesis is a simple hypothesis.\n",
    "A composite hypothesis is when a null or alternative hypothesis where the parameter of interest can take on more than one value. Above, the alternative was a composite hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power\n",
    "\n",
    "\n",
    "When we make a decision based on this, there are two undeal situtations:\n",
    "\n",
    "1. Reject $H_{0}$ when $H_{0}$ is true (**Type I Error**)\n",
    "2. Fail to reject $H_{0}$ when the $H_{0}$ is false and should be rejected (**Type II Error**)\n",
    "\n",
    "Fortunately, if with conduct a test with $\\alpha$ significance level, the probability of committing a Type I Error is controlled by $\\alpha$, $\\mathbb{P}(\\text{Type I Error}) \\leq \\alpha$.\n",
    "\n",
    "Thus, when $\\alpha$ is low enough, the probability that we reject the null when we should not is low. Now, we want the probability of committing a Type II Error to be as low as possible. The power of a test is $\\text{Power} = \\mathbb{P}(\\text{reject } H_{0} | H_{1}) = 1 - \\mathbb{P}(\\text{fail to reject } H_{0}|H_{1}) = 1 - \\mathbb{P}(\\text{Type II Error})$; thus, we want our power to be high.\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "Let $X_{1}, \\dots, X_{n} \\sim \\text{Bernoulli}(p)$, with $0 < p < 1$ unknown.\n",
    "\n",
    "Let our hypotheses be:\n",
    "$$H_{0}: p = p_{0}$$ \n",
    "$$H_{1}: p > p_{0}$$\n",
    "and our test statistic under the $H_{0}$ is:\n",
    "$$T(X_{1}, \\dots, X_{n}) = \\frac{\\hat{p} - p_{0}}{\\sqrt{\\frac{p_{0}(1-p_{0})}{n}}}$$\n",
    "and $\\hat{p} = \\frac{1}{n}\\sum_{i=1}^{n}X_{i}$\n",
    "Under $H_{0}$, we can show that $T \\sim N(0,1)$\n",
    "What is the type I and type II error rate?\n",
    "\n",
    "The type I error rate is $\\alpha$.\n",
    "The type II error rate is: $\\mathbb{P}(\\text{Type II error}) = \\mathbb{P}(\\text{fail to reject } H_{0}|H_{1})$. Under $H_{1}$, $p$ can take on any value in the interval $(p_{0}, 1]$\n",
    "\n",
    "Then, we can write the probability of making a type II error as: $\\mathbb{P}(\\text{Type II Error}) = \\mathbb{P}(\\text{fail to reject } H_{0}|p = p_{1})$.  Since there are multiple $p_{1}$, we write the type II error as:\n",
    "\n",
    "$$\\mathbb{P}(\\text{Type II Error}) = \\mathbb{P}(T(X_{1},\\dots,X_{n}) < z_{1-\\alpha}|p=p_{1}) = \\mathbb{P}\\left( \\frac{\\hat{p} - p_{0}}{\\sqrt{\\frac{p_{0}(1-p_{0})}{n}}} < z_{1-\\alpha}|p=p_{1}\\right)$$\n",
    "\n",
    "Under the specific alternative hypothesis $p = p_{1}$, we know that $\\frac{\\hat{p} - p_{1}}{\\sqrt{\\frac{p_{1}(1-p_{1})}{n}}} \\sim N(0, 1)$\n",
    "\n",
    "After some algebra, we can write, $\\frac{\\hat{p} - p_{1}}{\\sqrt{\\frac{p_{1}(1-p_{1})}{n}}} < \\frac{(p_{0} - p_{1})\\sqrt{n}}{\\sqrt{p_{1}(1-p_{1})}} + z_{1-\\alpha} \\sqrt{\\frac{p_{0}(1-p_{0})}{p_{1}(1-p_{1})}}$ and as a result,\n",
    "\n",
    "$$\\mathbb{P}(\\text{Type II Error}) = \\mathbb{P}\\left(  \\frac{\\hat{p} - p_{1}}{\\sqrt{\\frac{p_{1}(1-p_{1})}{n}}} < \\frac{(p_{0} - p_{1})\\sqrt{n}}{\\sqrt{p_{1}(1-p_{1})}} + z_{1-\\alpha} \\sqrt{\\frac{p_{0}(1-p_{0})}{p_{1}(1-p_{1})}} \\bigg| \\thinspace p=p_{1}\\right) = \\Phi \\left( \\frac{(p_{0} - p_{1})\\sqrt{n}}{\\sqrt{p_{1}(1-p_{1})}} + z_{1-\\alpha} \\sqrt{\\frac{p_{0}(1-p_{0})}{p_{1}(1-p_{1})}}  \\right)$$\n",
    "\n",
    "Then, also, power is\n",
    "\n",
    "$$\\text{power }  = 1 - \\Phi \\left( \\frac{(p_{0} - p_{1})\\sqrt{n}}{\\sqrt{p_{1}(1-p_{1})}} + z_{1-\\alpha} \\sqrt{\\frac{p_{0}(1-p_{0})}{p_{1}(1-p_{1})}}  \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of tests\n",
    "\n",
    "Notice that for the following, we can modify the p-value calculation based on the alternative hypothesis.\n",
    "\n",
    "\n",
    "## Z-test\n",
    "\n",
    "\n",
    "### One-sample z-test\n",
    "\n",
    "\n",
    "Consider a sample $X_{1},..., X_{n}$, and we are given the population standard deviation.\n",
    "We want to test for following hypothesis: $H_{0}: \\mu = \\mu_{0}$ and $H_{1}: \\mu \\neq \\mu_{0}$. Then, we can perform a z-test where $Z = \\frac{ {\\bar X} - \\mu_{0} }{ \\sigma / \\sqrt{n} }$ where, under the null hypothesis, follows a $N(0, 1)$.\n",
    "\n",
    "The p-value is thus ${\\mathbb P}\\left( | Z |  > z_{\\alpha/2} \\bigg| H_{0} \\right)$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Two-sample z-test\n",
    "\n",
    "Consider a sample $X_{11},..., X_{1n}$ from one population and and $X_{21},..., X_{2n}$ from another population.\n",
    "\n",
    "Use this test when we wish to determine if the means of two populations are equal and we know the population standard deviations.\n",
    "\n",
    "Our test statistic is: $Z = \\frac{ ({\\bar X}_{1} - {\\bar X}_{2}) - (\\mu_{1} - \\mu_{2} ) }{ \\sqrt{ \\frac{\\sigma_{1}^{2}}{n_{1}} + \\frac{ \\sigma_{2}^{2} }{n_{2}} } }$\n",
    "\n",
    "Under $H_{0}$, this follows a $N(0, 1)$.\n",
    "\n",
    "The p-value is thus ${\\mathbb P}\\left( | Z |  > z_{\\alpha/2} \\bigg| H_{0} \\right)$.\n",
    "\n",
    "\n",
    "## T-test\n",
    "\n",
    "Consider a sample $X_{1},..., X_{n}$, but now we do not know the population standard deviation. Since we introduce another measure of uncertainty, then we estimate $\\sigma$ with $\\hat \\sigma$. Then, we can perform a t-test where $T = \\frac{ {\\bar X} - \\mu_{0} }{ {\\hat \\sigma} / \\sqrt{n} }$ where, under the null hypothesis, follows a $t_{n-2}$.\n",
    "\n",
    "\n",
    "The p-value is thus ${\\mathbb P}\\left( | T |  > t_{\\alpha/2, n-2} \\bigg| H_{0} \\right)$.\n",
    "\n",
    "\n",
    "We can modify the p-value calculation based on the alternative hypothesis.\n",
    "\n",
    "In order for the test statistic to have a t-distribution, the data needs to be normal, but if the data is not normal, the test statistic has an approximate t-distribution when we have a sample size of at least 30.\n",
    "\n",
    "\n",
    "### Two-sample t-test (pooled variance)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Two-sample t-test (unpooled variance)\n",
    "\n",
    "\n",
    "\n",
    "## Likelihood Ratio Tests\n",
    "\n",
    "In the case of simple hypotheses, we can create a likelihood ratio test between them. In other words, we can consider a simple-vs.-simple hypothesis test for parameter $\\theta$: \n",
    "\n",
    "$H_{0}: \\theta = \\theta_{0}$\n",
    "\n",
    "$H_{1}: \\theta = \\theta_{1}$\n",
    "\n",
    "Then, we can write the likelihood-ratio test statistic as: $\\Lambda = \\frac{ {\\mathcal L} ( \\theta_{1} | x) }{ {\\mathcal L} (\\theta_{0} | x)}$. \n",
    "Here, we can think of this as comparing between two models. The likelihood-ratio test thus gives the following decision rule:  \n",
    "\n",
    "- If $\\Lambda > c$, reject $H_{0}$ \n",
    "- Otherwise, reject $H_{0}$\n",
    "\n",
    "Notice that other sources will may consider the reciprocal of the $\\Lambda$ defined here, in which case our decision rule is also switched.\n",
    "\n",
    "\n",
    " - Neyman-Pearson Lemma: Any other simple test with significance level $\\alpha' \\leq \\alpha$ has a power less than or equal to that of the LRT. In other words, when we have simple hypotheses, the LRT is the most powerful test at that significance level among tests with simple hypotheses.\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "Let $X_{1},\\dots, X_{n} \\overset{i.i.d}{\\sim} \\text{Poisson}(\\lambda)$ with $\\lambda > 0$. Furthermore, let $H_{0}: \\lambda = \\lambda_{0}$ and  $H_{1}: \\lambda = \\lambda_{1}$. Also, assume that $\\lambda_{1} > \\lambda_{0}$.\n",
    "\n",
    "Our likelihood function is, for some $\\lambda$:\n",
    "\n",
    "$\\mathcal{L}(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda}\\lambda^{X_{i}}}{X_{i}!}= \\frac{\\exp \\left\\{ -n\\lambda \\right\\} \\lambda^{\\sum_{i=1}^{n} X_{i}}}{ \\prod_{i=1}^{n} X_{i} ! }$\n",
    "\n",
    "Then, the likelihood-ratio test statistic is\n",
    "\n",
    "$$\\Lambda = \\frac{ {\\mathcal L} ( \\lambda_{1} | x) }{ {\\mathcal L} (\\lambda_{0} | x)} = \\frac{\\exp \\left\\{ -n\\lambda_{1} \\right\\} \\lambda_{1}^{\\sum_{i=1}^{n} X_{i}}}{ \\prod_{i=1}^{n} X_{i} ! } \\frac{ \\prod_{i=1}^{n} X_{i} !  }{ \\exp \\left\\{ -n\\lambda_{0} \\right\\} \\lambda_{0}^{\\sum_{i=1}^{n} X_{i}}  } = \\exp\\left\\{ -n (\\lambda_{1} - \\lambda_{0}) ) \\right\\} \\left( \\frac{\\lambda_{1}}{\\lambda_{0}} \\right)^{\\sum_{i=1}^{n} X_{i} }$$\n",
    "\n",
    "Notice that when $\\Lambda$ is high, then, this means it is much more likely for our data to come from the alternative distribution as opposed to the null distribution. Since $\\lambda_{1} > \\lambda_{0}$, the likelihood ratio is large when $\\sum_{i=1}^{n} X_{i}$ is also large, and we want to reject when $\\Lambda > b$ for some $b$. Then, we can simply consider $\\sum_{i=1}^{n} X_{i} > c$ for some $c$. We know that the sum of Poisson is also Poisson with rate $n\\lambda$. Then, our rejection region is: $\\sum_{i=1}^{n} X_{i} > c$ where for a $\\alpha$ significance level test, $c$ is the $1-\\alpha$ quantile of a $Pois(n\\lambda)$. Under $H_{0}$, it is $Pois(n\\lambda_{0})$, and under $H_{1}$, it is $Pois(n\\lambda_{1})$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Note: There are different variations of the likelihood ratio test, including a generalized ratio test, which considers a set of values that a parameter can take on under the entire possible set of values and under the null hypothesis.\n",
    "\n",
    "Under the generalized likelihood ratio test, we have the following set up: $H_{0}: \\theta \\in \\Theta_{0}$, and our likelihood ratio test statistic is $\\lambda = 2 \\log\\left( \\frac{ \\sup_{\\theta \\in \\Theta} {\\mathcal L}(\\theta)  }{ \\sup_{\\theta \\in \\Theta_{0}} {\\mathcal L}(\\theta) } \\right)=2 \\log\\left( \\frac{{\\mathcal L} }{{ \\mathcal L {\\hat \\theta} }({\\hat \\theta}_{0}) } \\right)$\n",
    "where ${\\hat \\theta}$ is the MLE under the entire space and $\\hat \\theta_{0}$ is the MLE under the restricted space under the null.\n",
    "\n",
    "Furthermore, there is a useful theorem about the limiting distribution of the LRT. \n",
    "\n",
    "Let $\\theta = (\\theta_{1},..., \\theta_{q}, \\theta_{q+1},..., \\theta_{r})$ and say we want to test the null, $\\Theta_{0}: (\\theta_{q+1},..., \\theta_{r} ) = \\theta_{0, q+1},..., \\theta_{0, r}$. Then $\\lambda \\rightarrow \\chi^{2}_{r-q, \\alpha}$ under $H_{0}$.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use\n",
    "\n",
    "We use z-tests and t-tests when we want to do a one-sample or two-sample tests, and if we know the variance, we opt for the z-test. Generally, however, we do not, so we estimate the variance and use the use a t-test to account for this added uncertainty. Likelihood ratio tests are used when we are comparing simple hypotheses and tests for the goodness-of-fit between two models. Generalized likelihood ratio tests is a general method for testing composite hypotheses.\n",
    "\n",
    "Notice that z-tests and t-tests in particular are limited to comparing at most 2 groups. \n",
    "\n",
    "When we want to test for more than two groups, then we consider ANOVA, MANOVA, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA\n",
    "\n",
    "ANOVA is used to determine whether or not there is a statistically significant difference between means of at least 2 independent groups. ANOVA is used when we are dealing with at least two categories and comparing based on a continuous outcome variable. For instance, perhaps we want to examine if at least 2 different classroom setups lead to different average exam scores. \n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\chi^{2}$ tests\n",
    "\n",
    "\n",
    "### Goodness-of-fit $\\chi^{2}$ test\n",
    "\n",
    "\n",
    "\n",
    "### $\\chi^{2}$ test for Independence\n",
    "\n",
    "\n",
    "### $\\chi^{2}$ test for Homogeneity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "Wasserman, L. (2010). All of statistics: a concise course in statistical inference. New York: Springer. ISBN: 9781441923226 1441923225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
