---
title: "A collection of proofs"
author: "Sharon Hui"
date: "1/18/2024"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Exercises

This section has almost nothing to do with Stat 135. It is a collection of proofs and exercises of useful facts/theorems. Personally, for me to truly believe and understand a statistical theorem, I need to see and understand the proof. Additionally, since I don't have the luxury of being a student anymore, having this section here keeps me accountable for always improving my proof-writing skills. If there are errors, please let me know. 


As a personal philosophy, I believe that we are all models. We learn skills through examples and practice, just like a model learns to predict and classify through training data. Another thing I would like to say is that a model may be penalized for getting the answer wrong, but it doesn't stop trying to learn. It only really stops learning when it runs out of examples to learn. 

What makes humans so incredible is that we are the vessels for many learning tasks. Given that we have this unique advantage over modeling, we have to keep learning and practice to get better at our crafts. With that being said, I'll be sporadically updating this page with problems/proofs.


Happy learning!



## Problem 1: Pairwise Euclidean distances and variance

Consider using Euclidean distances to measure how far points are from each other.

Show that the sum of all pairwise distances between a sample of individuals is directly related to variance.


$$\sum_{i=1}^{n}\sum_{\ell=1}^{n} d^{2}(i, \ell) = (2n^{2}) \sum_{i=1}^{n} d^{2}(i,g)$$

Here, $g$ is the centroid.

Solution:

What we are really showing is:

$$\sum_{i=1}^{n}\sum_{\ell=1}^{n} (x_{i} - x_{\ell})^2 = 2n \sum_{i=1}^{n} (x_i - \bar{x})^2$$


From the left side:

$$\sum_{i=1}^{n}\sum_{\ell=1}^{n} \left[x_{i}^2 - 2x_{i}x_{\ell} + x_{\ell}^2\right]= \sum_{i=1}^{n} \left(n x_{i}^{2} - 2x_{i}\sum_{\ell=1}^{n}  x_{l} + \sum_{\ell=1}^{n}x_{\ell}^{2} \right) = \sum_{i=1}^{n} \left(n x_{i}^{2} - 2x_{i} n \bar x + \sum_{\ell=1}^{n}x_{\ell}^{2} \right)$$

$$n\sum_{i=1}^{n}  x_{i}^{2} - 2n{\bar x} n \bar{x} + n \sum_{\ell=1}^{n} x_{\ell}^{2} = 2n \sum_{i=1}^{n}  x_{i}^{2} - 2n^{2} \bar x^2 = 2n\left[\sum_{i=1}^{n}  x_{i}^{2}  -  n \bar {x}^2 \right] = 2n \sum_{i=1}^{n} (x_{i} - \bar{x})^{2}$$

Notice  $\sum_{i=1}^{n} (x_{i} - \bar{x})^{2} = \sum_{i=1}^{n} x_{i}^{2} - 2{\bar {x}} n \bar {x} + n \bar {x}^{2} = \sum_{i=1}^{n} x_{i}^{2} - n{\bar x}^{2}$

This ultimately shows us that minimizing the variance is the same as minimizing the pairwise distance.

## Centering matrix

When you multiply the centering matrix ($\bf C_n=I_{n}-{\tfrac {1}{n}}J_{n}$ where $I_{n}$, is the identity matrix of size $n$ and $J_n$ is an $n$-by-$n$ matrix of all 1's.) with a vector, it is effectively the same as subtracting the mean of the components of the vector from every component of that vector. Prove the following: The centering matrix is a symmetric and idempotent matrix; this is called being a projection matrix. The centering matrix is also positive semi-definite. 


First, let's prove $\bf C_n$ is symmetric

I want to show that $\bf C_n$ is symmetric, which means that ${\bf C_n}^T = \bf C_n$

Using the properties of a transpose, notice that $({\bf {A-B}})^T ={\bf { A}}^T - {\bf {B}}^T$

$${\bf C_n}^T = \left({\bf I_n} - \frac{1}{n} \bf{1} \bf{1}^T \right) ^T =  ({\bf I_n})^T - \left(\frac{1}{n} \bf{1} \bf{1}^T \right)^T = {\bf I_n} - \frac{1}{n} (\bf{1}^T)^T (\bf{1})^T$$
$$= \left({\bf I_n} - \frac{1}{n} \bf{1} \bf{1}^T \right) = {\bf C_n}$$
Since ${\bf C_n}^T = {\bf C_n}$, this means that ${\bf C_n}$ is symmetric.

Next, let's show $\bf C_n$ is idempotent

I want to show that $\bf C_n$ is idempotent, which means that ${\bf C_n}^2 = \bf C_n$. 

Firstly, notice that ${\bf 1}$ is a n by 1 vector:

$${\bf 1}=\begin{bmatrix}
    1 \\
    1\\
    \vdots \\
    1
    \end{bmatrix}$$
$${\bf 1}^T=\begin{bmatrix}
    1 & 1 & \cdots & 1
    \end{bmatrix}$$
This means that 

$${\bf 1}^T{\bf 1}= \begin{bmatrix}
    1 & 1 & \cdots & 1
    \end{bmatrix}
    \begin{bmatrix}
    1 \\
    1\\
    \vdots \\
    1
    \end{bmatrix} = \sum_{i=1}^n 1=n$$

$${\bf C_n}^2 = {\bf C_n}{\bf C_n}  = ({\bf I}_n - \frac{1}{n} {\bf{1}} {\bf{1}}^T)({\bf I}_n - \frac{1}{n} {\bf{1}} {\bf{1}}^T)$$
$$= {\bf I_n I_n} - {\bf I_n}\frac{1}{n}{{\bf 1} {\bf 1}^T} - \frac{1}{n}{{\bf 1} {\bf 1}^T}{\bf I_n} + \frac{1}{n^2}{{\bf 1} {\bf 1}^T}{{\bf 1} {\bf 1}^T}$$
$$= {\bf I_n I_n} - {\bf I_n}\frac{1}{n}{{\bf 1} {\bf 1}^T} - \frac{1}{n}{{\bf 1} {\bf 1}^T}{\bf I_n} + \frac{1}{n^2}{{\bf 1}} n {{\bf 1}^T}={\bf I_n I_n} - \frac{1}{n}{{\bf 1} {\bf 1}^T} - \frac{1}{n}{{\bf 1} {\bf 1}^T} + \frac{1}{n}{{\bf 1}}  {{\bf 1}^T}$$
$$= {\bf I_n} - \frac{1}{n}{{\bf 1} {\bf 1}^T} ={\bf C_n}$$

This shows that ${\bf C_n}^2 = {\bf C_n}$.

Lastly, let's show ${\bf C_n}$ is PSD.

For ${\bf C_n}$ to be PSD, it means for all non-zero ${\bf x} \in {\mathbb R}$, ${\bf x}^{T} {\bf C_n} {\bf x} \geq 0$


$${\bf x}^{T} \left( {\bf I_n} - \frac{1}{n}{{\bf 1} {\bf 1}^T}\right) {\bf x}$$

$${\bf x}^{T} {\bf x} - \frac{1}{n} {\bf x}^{T} {{\bf 1} {\bf 1}^T} {\bf x} = \sum_{i=1}^{n} x_{i}^{2} - \frac{1}{n} \sum_{i=1}^{n} x_{i} \sum_{i=1}^{n} x_{i}=\sum_{i=1}^{n} x_{i}^{2} -  n {\bar x}^{2}=\sum_{i=1}^{n} (x_{i} - \bar{x})^{2} \geq 0$$
Notice  $\sum_{i=1}^{n} (x_{i} - \bar{x})^{2} = \sum_{i=1}^{n} x_{i}^{2} - 2{\bar {x}} n \bar {x} + n \bar {x}^{2} = \sum_{i=1}^{n} x_{i}^{2} - n{\bar x}^{2}$

Since ${\bf C_n}$ are PSD, the eigenvalues of ${\bf C_n}$ are either zero or one.

Alternatively, we can show it by using the idempotency of ${\bf C_n}$. 
Let ${\bf C_n} {\bf v_i}= \lambda_i {\bf v_i}$ where ${\bf v_i}$ is a eigenvector and $\lambda_i$ is its corresponding eigenvalue.

$$\lambda_i {\bf v_i} = {\bf C_n} {\bf v_i} = {\bf C_n}^2 {\bf v_i} = {\bf C_n}{\bf C_n} {\bf v_i} = {\bf C_n}\lambda_i {\bf v_i} = \lambda_i {\bf C_n} {\bf v_i} = \lambda_i\lambda_i {\bf v_i}= \lambda_i^2 {\bf v_i}$$

This means that $\lambda_i {\bf v_i} = \lambda_i^2 {\bf v_i}$ must hold true, which is only when $\lambda_i$ is 0 or 1.

## Symmetric Matrices


Eigenvectors of real symmetric matrices are orthogonal.

If A is symmetric, then $\bf A = \bf A^T$. By definition, $\bf A \bf v_i = \lambda_i \bf v_i$ and $\bf A \bf v_j = \lambda_j \bf v_j$.

$$\lambda_i {\bf v_i^T}{\bf v_j}={\bf (\lambda_i v_i)^T}{\bf v_j}={\bf (A v_i)^T}{\bf v_j}= {\bf v_i^T} {\bf A^T} {\bf v_j}={\bf v_i^T} ({\bf A^T} {\bf v_j})={\bf v_i^T} ({\bf A} {\bf v_j})={\bf v_i^T} (\lambda_j {\bf v_j})=\lambda_j{\bf v_i^T} ( {\bf v_j})$$

$$\lambda_i {\bf v_i^T}{\bf v_j}=\lambda_j{\bf v_i^T} ( {\bf v_j})$$

This means that

$$\lambda_i {\bf v_i^T}{\bf v_j}-\lambda_j{\bf v_i^T}  {\bf v_j}=0$$

Since ${\lambda_i}$ and ${\lambda_j}$ are two distinct eigenvalues meaning ${\lambda_i} \neq {\lambda_j}$, this must imply that ${\bf v_i^T}{\bf v_j}=0$


## Linear Regression

Assume that the following linear relationship is true: $Y = X\beta + \epsilon$, where $\epsilon \sim N(0, \sigma^{2})$. 

Let $Y =\begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix}$ and $X=\begin{bmatrix} x_{11} & x_{12} & \cdots x_{1p} \\ \vdots \\ x_{n1} & x_{n2} & \cdots x_{np} \end{bmatrix}$, so $X$ is a $n \times p$ matrix.

We want to find the best linear fit to the data so ${\hat Y} = X {\hat \beta}$. Choosing that we want to minimize the squared loss, then we want to minimize $$\| {Y} - {\hat Y}  \| = \| {Y} - {X {\hat \beta}} \|$$. Then $$({Y} - {X {\hat \beta}})^{T}({Y} - {X {\hat \beta}}) = Y^{T}Y - {\hat \beta}^{T} X^{T}Y - Y^{T}X{\hat \beta} + {\hat \beta}^{T} X^{T} X {\hat \beta} = 
Y^{T}Y - 2 {\hat \beta}^{T} X^{T}Y + {\hat \beta}^{T} X^{T} X {\hat \beta}$$
Taking derivative with respect to $\hat \beta$ and set to $0$: $$X^{T}Y= X^{T} X {\hat \beta} \longrightarrow {\hat \beta } = (X^{T}X)^{-1}X^{T}Y$$
Know that this requires $X^{T}X$ to be invertible, which happens when $X$ is full rank.

$$E[Y] = E[X\beta + \epsilon] = X \beta$$
$$Cov(Y) = Cov(X\beta + \epsilon) = Cov(\epsilon) = \sigma^{2}$$

$$E[{\hat \beta}] = E[(X^{T}X)^{-1}X^{T}Y] = (X^{T}X)^{-1}X^{T} X \beta = \beta$$

$$Var({\hat \beta})= Cov({\hat \beta}, {\hat \beta}) = Cov((X^{T}X)^{-1}X^{T}Y, (X^{T}X)^{-1}X^{T}Y) = (X^{T}X)^{-1}X^{T} Cov(Y,Y) ((X^{T}X)^{-1}X^{T})^{T}$$
$$=(X^{T}X)^{-1}X^{T} \sigma^{2} X (X^{T}X)^{-1} = \sigma^{2} (X^{T}X)^{-1}$$

## Weighted Least Squares


Weighted least squares is used when linear measurements are corrupted by errors with unequal variances. We want to minimize $\sum_{i=1}^{n} w_{i} (Y_{i} - \sum_{j=1}^{p} x_{ij} \beta_{j})^{2}$ with the constraint $w_{1},..., w_{n} \geq 0$

In matrix form, the equivalent problem is ${\arg\min}_{\beta} \|W^{1/2} (Y-X\beta) \|^{2}$, with $W$ being a symmetric, positive-definite weighting matrix.

The normal equations are then:

$$(W^{1/2} (Y-X\beta) )^{T} (W^{1/2} (Y-X\beta) ) = (Y^{T} - \beta^{T} X^{T}) (W^{1/2})^{T} W^{1/2} (Y-X\beta)$$
$$= Y^{T} W Y - \beta^{T} X^{T} W Y - Y^{T} WX \beta + \beta^{T} X^{T} W X \beta$$

Take derivative wrt $\beta$ and set to $0$:

$$2X^{T} W Y = 2X^{T} W X \beta \longrightarrow {\hat \beta} = (X^{T} W X)^{-1} X^{T}W Y$$

Weighted least squares is essentially the same as transforming $X$ and $Y$ and $\epsilon$.

The main advantage of using WLS is the retainment of interpretations of the coefficients and interpreting F-tests and R-squared values (usually WLS keeps the intercept while transformed models do not). It is also advantageous to downweight outlier or influential points by setting its weight to 0.


## Correlation Matrix


The correlation matrix is a symmetric PSD matrix with unit diagonal.


Covariance between two random variables, $X$ and $Y$: $Cov(X, Y) = {\mathbb E}[(X-{\mathbb E}(X))(Y-{\mathbb E}(Y))] = {\mathbb E}[XY - Y{\mathbb E}(X) - X {\mathbb E}(Y) + {\mathbb E}(X) {\mathbb E}(Y) ] = {\mathbb E}(XY) - {\mathbb E}(X) {\mathbb E}(Y)$, provided that the expectation exists.



## Least Squares with Regularization

### Ridge Regression

With regularization, extra terms (variables) are added to the cost function (to avoid overfitting). 
$$\min \sum_{i=1}^{n} (y_{i} - x_{i^{T}}\beta)^{2} + \rho \sum_{j=1}^{p} \beta_{j}^{2}$$
with the constraint that $\rho > 0$.


### Lasso Regression

With regularization, extra terms (variables) are added to the cost function (to avoid overfitting). 
$$\min \sum_{i=1}^{n} (y_{i} - x_{i^{T}}\beta)^{2} + \lambda \sum_{j=1}^{p} |\beta_{j} |$$
with the constraint that $\lambda > 0$.

Alternatively, we can write $$\min \| Y-X\beta \|^{2}_{2} + \lambda \| \beta \|_{1}$$



