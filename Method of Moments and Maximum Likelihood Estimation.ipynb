{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "381d3c14",
   "metadata": {},
   "source": [
    "# Estimation\n",
    "\n",
    "An estimator is some statistic that gives some approximation about some fact/attribute about the population. How do we choose an appropriate estimator? What kind of estimator should we choose? Here, we will go over two methods to find estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db75f84d",
   "metadata": {},
   "source": [
    "# Method of Moments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f1c005",
   "metadata": {},
   "source": [
    "The method of moments is one of the oldest methods of deriving point estimators. Although they are not the best estimators usually, they almost always produce asymptotically unbiased esttimators. Furthermore, this method utilizes the substitution principle, an important idea in statistics. Additionally, in general, the method of moment estimators are consistent (${\\hat \\theta}_{MOM} \\overset{p }{\\rightarrow} \\theta$). As a result, it is worth understanding how the method of moments works. \n",
    "\n",
    "Suppose we want to estimate $k$ parameters, $\\theta_{1}, ... \\theta_{k}$ from a probability distribution. Furthermore, we have $f(x | \\theta_{1},..., \\theta_{k})$ from i.i.d. samples $X_{1},..., X_{n}$ from this distribution. We first compute the first $k$ moments and we express our parameters in terms of these moments. Then, our method of moment estimators are found by substituting these moments with sample moments. Since we are creating a system of equations, the number of equations that we need is the same as the number of parameters that we will estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b01efd",
   "metadata": {},
   "source": [
    "### Example: Method of Moments for Poisson Distribution\n",
    "\n",
    "Suppose we have $n$ observations, $X_{1},... ,X_{n}$ and each $X_{i} \\sim Pois(\\lambda)$. Find a MOM estimator for $\\lambda$.\n",
    "\n",
    "Let $X \\sim Pois(\\lambda)$, then $E[X] = \\lambda$. Then, our method of moments estimator is ${\\hat \\lambda}_{MOM} = {\\bar X}$. \n",
    "\n",
    "\n",
    "### Example: Method of Moments for Gamma Distribution\n",
    "\n",
    "Suppose we have $n$ observations, $X_{1},... ,X_{n}$ and each $X_{i} \\sim Gamma(r, \\lambda)$. Find a MOM estimator for $r$ and $\\lambda$.\n",
    "\n",
    "\n",
    "We know that $E[X] = \\frac{r}{\\lambda}$ and $Var(X) = \\frac{r}{\\lambda^{2}}$. Then $E[X^{2}] = \\frac{r}{\\lambda^{2} } + \\frac{r^{2}}{\\lambda^{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532c9f3d",
   "metadata": {},
   "source": [
    "$$E[X^{2}] = Var(X) + (E[X])^{2} = \\frac{1}{\\lambda} E[X] + (E[X])^{2}$$\n",
    "\n",
    "$$\\lambda = \\frac{E[X]}{E[X^{2}]  - (E[X])^{2} }$$\n",
    "\n",
    "$$r = \\frac{(E[X])^{2}}{E[X^{2}]  - (E[X])^{2} }$$\n",
    "\n",
    "Then our method of moments estimators are: \n",
    "\n",
    "$${\\hat \\lambda}_{MOM} = \\frac{{\\hat \\mu_{1}}}{ {\\hat \\mu_{2}} - {\\hat \\mu_{1}}^{2}}$$\n",
    "\n",
    "$${\\hat r} = \\frac{\\hat \\mu_{1}^{2}}{ \\hat \\mu_{2}  - \\hat \\mu_{1}^{2} }$$\n",
    "\n",
    "where $\\hat \\mu_{1} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i} =  {\\bar X}$ and $\\hat \\mu_{2} = \\frac{1}{n}{ \\sum_{i=1}^{n} X_{i}^{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a26cda",
   "metadata": {},
   "source": [
    "The weak law of large numbers implies that the sample moments converge (in probability) to population moments. Additionally, if we can represent our estimates as a continuous, smooth function of the sample moments, the estimates will also converge in probability to the parameters of interest. The weak law of large numbers implies that the sample moments converge (in probability) to population moments. Additionally, if we can represent our estimates as a continuous, smooth function of the sample moments, the estimates will also converge in probability to the parameters of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2952f72",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation\n",
    "\n",
    "Maximum likelihood estimation aims to maximize the likelihood function, denoted as $L = f_{X_{1},..., X_{n}}(x_{1},..., x_{n} | \\theta )$, such that under the model, the observed data is the most probable. A lot of the times, it is difficult to deal with maximizing the product, so we aim to maximize the log likelihood of a function instead. Since the log function is a monotonically increasing function on $x>0$. Thus, the logarithm of a function achieves its max value at the same point as the function. Denote $\\ell = \\log(L)$, where $\\log$ is the natural logarithm. Then, we can take the derivative and set to 0 and solve for $\\lambda$ to obtain our maximum likelihood estimate.\n",
    "\n",
    "Put simply, we find our maximum likelihood estimator as ${\\hat \\theta}_{ML} = \\arg \\max_{\\theta \\in \\Theta} L = \\arg \\max_{\\theta \\in \\Theta} \\ell$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ca1f43",
   "metadata": {},
   "source": [
    "### Example: Maximum Likelihood Estimation for Poisson Distribution\n",
    "\n",
    "$X_{1},..., X_{n}$ be i.i.d. $Pois(\\lambda)$. Then, we have that our likelihood function is\n",
    "\n",
    "$$L = f_{X_{1},..., X_{n}}(x_{1},..., x_{n} | \\theta ) = \\prod_{i=1}^{n} \\frac{\\lambda^{x_{i}} \\exp\\{ -\\lambda\\}}{x_{i}!}$$\n",
    "\n",
    "\n",
    "$$\\ell = \\log( \\lambda) \\sum_{i=1}^{n} x_{i} -\\sum_{i=1}^{n} \\lambda  - \\sum_{i=1}^{n}\\log( x_{i}!)$$\n",
    "\n",
    "$$\\frac{\\partial  \\ell }{ \\partial \\lambda} = \\frac{\\sum_{i=1}^{n} x_{i}}{\\lambda} -  n \\overset{set}{=}0$$\n",
    "\n",
    "Solving for ${\\lambda}$, we have that ${\\hat \\lambda}_{MLE} = {{\\bar X}}$\n",
    "\n",
    "Notice that our method of moments and maximum likelihood estimator agree in the case where we have a Poisson distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cc86b2",
   "metadata": {},
   "source": [
    "### Example: Maximum Likelihood Estimation for Exponential\n",
    "\n",
    "$X_{1},..., X_{n}$ be i.i.d. $Exp(\\lambda)$. Then, we have that our likelihood function is\n",
    "\n",
    "$$L = f_{X_{1},..., X_{n}}(x_{1},..., x_{n} | \\theta ) = \\prod_{i=1}^{n} \\lambda \\exp \\left\\{ -\\lambda x_{i} \\right\\}=\\lambda^{n} \\exp \\left\\{ -\\lambda \\sum_{i=1}^{n} x_{i} \\right\\}$$\n",
    "\n",
    "\n",
    "$$\\ell = n \\log \\lambda - \\lambda \\sum_{i=1}^{n} x_{i}$$\n",
    "\n",
    "$$\\frac{\\partial  \\ell }{ \\partial \\lambda} = \\frac{n}{\\lambda} -  \\sum_{i=1}^{n} x_{i}\\overset{set}{=}0$$\n",
    "\n",
    "Solving for ${\\lambda}$, we have that ${\\lambda}_{MLE} = \\frac{1}{{\\bar X}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43ae013",
   "metadata": {},
   "source": [
    "## Properties of MLE\n",
    "\n",
    "Under certain conditions, the MLE possesses several properties that make MLE more favorable than the method of moments estimator. They are:\n",
    "\n",
    "1. MLE is equivariant: Let ${\\hat \\theta}_{ML}$ be the MLE of $\\theta$. Then $f({\\hat \\theta}_{ML})$ is the MLE of $f(\\theta)$.\n",
    "\n",
    "2. MLE is consistent: ${\\hat \\theta}_{MLE} \\overset{ p  }{ \\rightarrow } \\theta$\n",
    "\n",
    "3. MLE is asymptotically normal: ${\\hat \\theta}_{ML} \\rightarrow {\\mathcal N} \\left(\\theta,\\frac{1}{n} \\left( I(\\theta) \\right)^{-1} \\right)$ where $I(\\theta)$ is the Fisher information. $I(\\theta) := E_{\\theta} \\left[ \\left( \\frac{\\partial}{ \\partial \\theta } \\log(f(x|\\theta)) \\right)^{2} \\right]$\n",
    "\n",
    "4. MLE is asymptotically unbiased $\\lim_{n \\rightarrow \\infty} E[ {\\hat \\theta}_{ML} ] = \\theta$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
